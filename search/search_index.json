{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Defensive Works","title":"Home"},{"location":"#defensive-works","text":"","title":"Defensive Works"},{"location":"aws/","text":"Collection of ideas, techniques, automations` to help with defending your AWS Platform and workloads at different layers and mechanisms","title":"AWS"},{"location":"aws/general/cdk-bootstrap/","text":"CDK Bootstrapping at scale Born out of love for AWS CDK. Bootstrap all AWS Accounts and their regions under the AWS Organization and also add a trusted account which could further be the CI/CD or CDK Pipeline account from which automation is run later. This will be a one time setup to be done which will immensly help in laying the foundation for Cross Account CDK Pipelines Disclaimer This automation process is one of the many many ways to accomplish the end result of bootstrapping all Accounts in an AWS Organization Prerequisites cdk should be preinstalled. If not run npm install -g aws-cdk in your terminal. Have access to the AWS Organization Account with an IAM Role or user and those credentials set in the current shell before running this program. Workflow The program uses the AWS Organization AWS Profile credentials to list_accounts under the organization. This information is used to assume the OrganizationAccountAccessRole in each of those accounts`. Once a session is assumed, the session credentials are exported to the local shell. With the current shell environment variables set for the particular AWS Account, it goes ahead and bootstraps that account & every region in it. What is bootstrapping Bootstrapping is basically a cloudformation template CDKToolkit which creates necessary resources such as s3 buckets, ecr registries, IAM roles, etc.. in place for further deployments using CDK. Usage Git clone repository git clone https://github.com/raajheshkannaa/cdk-booty-strappin If using an IAM User/role set AWS Organiztion profile in ~/.aws/credentials file. If using SSO such as Okta for the AWS Orgnization account, use a program such as gimmeawscreds or saml2aws to obtain credentials and either set those as env variables or as a profile and export as AWS_PROFILE=aws-org-profile . Confirm credentials with aws sts get-caller-identity . Open the python file and add your TRUSTED_ACCOUNT variable to match your CI/CD pipeline AWS Account, so that it is trusted as a source for further CDK Deployments. Run python3 cdk_bootstrap_multiple_accounts.py Notes This python code will only bootstrap member AWS Accounts for an AWS Organization, does not bootstrap the Organization Master itself. Considering you have existing access, please bootstrap the Billing account using cdk bootstrap --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess The reason this needs to be run from the AWS Organization Account, is because it has existing trust relationships with all the member aws accounts with the OrganizationAccountAccessRole . This could also be run from the SecurityAudit account in the case when you have Control Tower setup and all AWS Accounts enrolled with the trust relationship setup for the ControlTowerExecution role. The reason why we use subprocess instead of os from python is because when a subsequent or multiple commands are run with os.system back to back, they are run in separate child shell processes, making it hard to export/set environment variables, that the code could then make use to assume IAM roles with. The reason for doing this python and shell fu is that cdk bootstrap currently is a cli command and gathering session information and storing credentials in memory with python provides flexibility with different platforms such as AWS Lambda, local environments such as Windows, Mac or Linux. This could technically be run as a lambda function( not tested ) as well in the Organization Account whenever there is a new account is created, with help from CloudWatch Events. The Lambda function needs to be run with the OrganizationAccountAccessRole, after modifiying the trust of that role to be used by lambda to assume the role. Ideally I'm a big advocate of not running any resources in the AWS Organization Account and keep it locked as much as possible, however for the use case of accessing all memeber accounts from a central location is only possible from the Org account, until we establish the capability to do so from our Automation or Security Account after we establish a framework of necessary IAM Roles with trust relationships setup, like in a Control Tower environment. If you liked this, then you can buy me my first coffee ever, THANKS IN ADVANCE","title":"CDK Bootstrapping at scale"},{"location":"aws/general/cdk-bootstrap/#cdk-bootstrapping-at-scale","text":"Born out of love for AWS CDK. Bootstrap all AWS Accounts and their regions under the AWS Organization and also add a trusted account which could further be the CI/CD or CDK Pipeline account from which automation is run later. This will be a one time setup to be done which will immensly help in laying the foundation for Cross Account CDK Pipelines","title":"CDK Bootstrapping at scale"},{"location":"aws/general/cdk-bootstrap/#disclaimer","text":"This automation process is one of the many many ways to accomplish the end result of bootstrapping all Accounts in an AWS Organization","title":"Disclaimer"},{"location":"aws/general/cdk-bootstrap/#prerequisites","text":"cdk should be preinstalled. If not run npm install -g aws-cdk in your terminal. Have access to the AWS Organization Account with an IAM Role or user and those credentials set in the current shell before running this program.","title":"Prerequisites"},{"location":"aws/general/cdk-bootstrap/#workflow","text":"The program uses the AWS Organization AWS Profile credentials to list_accounts under the organization. This information is used to assume the OrganizationAccountAccessRole in each of those accounts`. Once a session is assumed, the session credentials are exported to the local shell. With the current shell environment variables set for the particular AWS Account, it goes ahead and bootstraps that account & every region in it.","title":"Workflow"},{"location":"aws/general/cdk-bootstrap/#what-is-bootstrapping","text":"Bootstrapping is basically a cloudformation template CDKToolkit which creates necessary resources such as s3 buckets, ecr registries, IAM roles, etc.. in place for further deployments using CDK.","title":"What is bootstrapping"},{"location":"aws/general/cdk-bootstrap/#usage","text":"Git clone repository git clone https://github.com/raajheshkannaa/cdk-booty-strappin If using an IAM User/role set AWS Organiztion profile in ~/.aws/credentials file. If using SSO such as Okta for the AWS Orgnization account, use a program such as gimmeawscreds or saml2aws to obtain credentials and either set those as env variables or as a profile and export as AWS_PROFILE=aws-org-profile . Confirm credentials with aws sts get-caller-identity . Open the python file and add your TRUSTED_ACCOUNT variable to match your CI/CD pipeline AWS Account, so that it is trusted as a source for further CDK Deployments. Run python3 cdk_bootstrap_multiple_accounts.py","title":"Usage"},{"location":"aws/general/cdk-bootstrap/#notes","text":"This python code will only bootstrap member AWS Accounts for an AWS Organization, does not bootstrap the Organization Master itself. Considering you have existing access, please bootstrap the Billing account using cdk bootstrap --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess The reason this needs to be run from the AWS Organization Account, is because it has existing trust relationships with all the member aws accounts with the OrganizationAccountAccessRole . This could also be run from the SecurityAudit account in the case when you have Control Tower setup and all AWS Accounts enrolled with the trust relationship setup for the ControlTowerExecution role. The reason why we use subprocess instead of os from python is because when a subsequent or multiple commands are run with os.system back to back, they are run in separate child shell processes, making it hard to export/set environment variables, that the code could then make use to assume IAM roles with. The reason for doing this python and shell fu is that cdk bootstrap currently is a cli command and gathering session information and storing credentials in memory with python provides flexibility with different platforms such as AWS Lambda, local environments such as Windows, Mac or Linux. This could technically be run as a lambda function( not tested ) as well in the Organization Account whenever there is a new account is created, with help from CloudWatch Events. The Lambda function needs to be run with the OrganizationAccountAccessRole, after modifiying the trust of that role to be used by lambda to assume the role. Ideally I'm a big advocate of not running any resources in the AWS Organization Account and keep it locked as much as possible, however for the use case of accessing all memeber accounts from a central location is only possible from the Org account, until we establish the capability to do so from our Automation or Security Account after we establish a framework of necessary IAM Roles with trust relationships setup, like in a Control Tower environment.","title":"Notes"},{"location":"aws/general/cdk-bootstrap/#if-you-liked-this-then-you-can-buy-me-my-first-coffee-ever-thanks-in-advance","text":"","title":"If you liked this, then you can buy me my first coffee ever, THANKS IN ADVANCE"},{"location":"aws/identity-access-management/fleetaccess/","text":"AWS Fleet Access at scale IAM Roles which will help in maintaining an AWS Multi-Account strategy at scale. Imagine a Hub & Spoke model, where we have a central automation or security AWS Account with the hub role and rest of the accounts(Fleet) with the spoke role. This can be achieved with AWS Control Tower , but what if I don't want to use Control Tower and spin up something of my own, or even further use this setup alongside Control Tower, for the role(s) used by CT, has AdministratorAccess privileges and that I want to use strictly constrained set of permissions for my roles in a multi-account setup. Enter FleetAccess . Pre-requisites The CDK pipeline/application itself is to be deployed in the Organization or Billing account, thus access is required for this account. The corresponding accounts, in our case, all accounts needs to be CDK bootstrapped Automated CDK Bootstrap all AWS Accounts with the Organization & Security/CICD account as trusted accounts, which means these accounts can deploy CloudFormation templates to the destination accounts. Infrastructure The infrastructure for this project is built with Cloud Development Kit or CDK. The primary reason is the self mutating capability of CDK Pipelines, secondary because the source for AAS Management is built with Python, building Infrastructure in the same language is a huge plus. If you don't have CDK setup locally already before deployment, please refer to the Official CDK page or install by running below command in a terminal. npm i -g aws-cdk Components Stacks We use CDK Pipelines to deploy the three stacks, * hub_role_stack is deployed in the Security or Automation account. * org_read_only_role_stack is deployed in the Organization or Billing account. * spoke_role_stack is deployed in all accounts', generally refered to as the fleet . Pipeline Codecommit repository holding the pipeline code itself, along with the stacks for the IAM roles. The CDK pipeline is a self mutating pipeline which is triggered whenever there is change to the Codecommit repository. The pipeline has two stages hub_org_deploy stage deploys, hub_role & org_read_only_role stacks, bundled together in a cross account fashion, as these are just 1 account each. spoke_deploy uses custom code which will invoke organizations:list_accounts api to gather all AWS Account IDs and then deploy the cloudformation stack in each of the accounts in parallel. Its` a satisfying sight to watch Usage git clone https://github.com/raajheshkannaa/fleet-access.git Open stacks/config.py file and edit to add your AWS Account IDs for the Hub role, which is the automation or security account, and the Organization/Billing Account ID. Once updated, run cdk ls , to list stacks and also make sure the cdk app has constructed the necessary stacks. This should result in an error, because we haven't deployed our cdk app yet. Export the organization account aws profile locally in a terminal using export AWS_PROFILE=aws-org-profile and then run cdk deploy . CDK will deploy the FleetAccessPipelineStack pipeline stack, which in turn creates the CodeCommit repository, CodePipeline with necessary IAM permissions, Stages of deployment. The first time you deploy, the pipeline runs and fails, because we haven't pushed our code yet. So, now gather the codecommit repository details which was created and git push the cdk app to the repository which has the cloudformation stacks to be deployed by the pipeline. Once code is pushed, pipeline picks it up automatically and begins deployment, its an amazing feeling to look at this in action, with so minimal effort. References If you want to build over this to futher tweak and customize the permissions of these roles, you can use CDK IAM Floyd You can use Codepipeline Slack Approval to have pull requests` approved from a slack channel, whenever you make changes to the IAM roles. If you liked this, then you can buy me my first coffee ever, THANKS IN ADVANCE","title":"AWS Cross Account Access at Scale"},{"location":"aws/identity-access-management/fleetaccess/#aws-fleet-access-at-scale","text":"IAM Roles which will help in maintaining an AWS Multi-Account strategy at scale. Imagine a Hub & Spoke model, where we have a central automation or security AWS Account with the hub role and rest of the accounts(Fleet) with the spoke role. This can be achieved with AWS Control Tower , but what if I don't want to use Control Tower and spin up something of my own, or even further use this setup alongside Control Tower, for the role(s) used by CT, has AdministratorAccess privileges and that I want to use strictly constrained set of permissions for my roles in a multi-account setup. Enter FleetAccess .","title":"AWS Fleet Access at scale"},{"location":"aws/identity-access-management/fleetaccess/#pre-requisites","text":"The CDK pipeline/application itself is to be deployed in the Organization or Billing account, thus access is required for this account. The corresponding accounts, in our case, all accounts needs to be CDK bootstrapped Automated CDK Bootstrap all AWS Accounts with the Organization & Security/CICD account as trusted accounts, which means these accounts can deploy CloudFormation templates to the destination accounts.","title":"Pre-requisites"},{"location":"aws/identity-access-management/fleetaccess/#infrastructure","text":"The infrastructure for this project is built with Cloud Development Kit or CDK. The primary reason is the self mutating capability of CDK Pipelines, secondary because the source for AAS Management is built with Python, building Infrastructure in the same language is a huge plus. If you don't have CDK setup locally already before deployment, please refer to the Official CDK page or install by running below command in a terminal. npm i -g aws-cdk","title":"Infrastructure"},{"location":"aws/identity-access-management/fleetaccess/#components","text":"","title":"Components"},{"location":"aws/identity-access-management/fleetaccess/#stacks","text":"We use CDK Pipelines to deploy the three stacks, * hub_role_stack is deployed in the Security or Automation account. * org_read_only_role_stack is deployed in the Organization or Billing account. * spoke_role_stack is deployed in all accounts', generally refered to as the fleet .","title":"Stacks"},{"location":"aws/identity-access-management/fleetaccess/#pipeline","text":"Codecommit repository holding the pipeline code itself, along with the stacks for the IAM roles. The CDK pipeline is a self mutating pipeline which is triggered whenever there is change to the Codecommit repository. The pipeline has two stages hub_org_deploy stage deploys, hub_role & org_read_only_role stacks, bundled together in a cross account fashion, as these are just 1 account each. spoke_deploy uses custom code which will invoke organizations:list_accounts api to gather all AWS Account IDs and then deploy the cloudformation stack in each of the accounts in parallel. Its` a satisfying sight to watch","title":"Pipeline"},{"location":"aws/identity-access-management/fleetaccess/#usage","text":"git clone https://github.com/raajheshkannaa/fleet-access.git Open stacks/config.py file and edit to add your AWS Account IDs for the Hub role, which is the automation or security account, and the Organization/Billing Account ID. Once updated, run cdk ls , to list stacks and also make sure the cdk app has constructed the necessary stacks. This should result in an error, because we haven't deployed our cdk app yet. Export the organization account aws profile locally in a terminal using export AWS_PROFILE=aws-org-profile and then run cdk deploy . CDK will deploy the FleetAccessPipelineStack pipeline stack, which in turn creates the CodeCommit repository, CodePipeline with necessary IAM permissions, Stages of deployment. The first time you deploy, the pipeline runs and fails, because we haven't pushed our code yet. So, now gather the codecommit repository details which was created and git push the cdk app to the repository which has the cloudformation stacks to be deployed by the pipeline. Once code is pushed, pipeline picks it up automatically and begins deployment, its an amazing feeling to look at this in action, with so minimal effort.","title":"Usage"},{"location":"aws/identity-access-management/fleetaccess/#references","text":"If you want to build over this to futher tweak and customize the permissions of these roles, you can use CDK IAM Floyd You can use Codepipeline Slack Approval to have pull requests` approved from a slack channel, whenever you make changes to the IAM roles.","title":"References"},{"location":"aws/identity-access-management/fleetaccess/#if-you-liked-this-then-you-can-buy-me-my-first-coffee-ever-thanks-in-advance","text":"","title":"If you liked this, then you can buy me my first coffee ever, THANKS IN ADVANCE"},{"location":"aws/incident-response/green-stone/","text":"Introduction Get notified of Security Group Changes across all AWS Accounts & Regions in an AWS Organization, with the ability to respond/revert those changes with a single button click from a Slack Channel. This is made easy and possible with the recent announcement of CloudTrail Lake, which helps aggregate CloudTrail logs from all accounts/regions in a queryable(if that's a word :P) format. The infrastructure needed for this project is deployed as a CDK Application, which deploys a CodeCommit repository and a CodeBuild stage which synthesizes the cdk app to cloudformation template and deploys in the target environment. NOTE Security Group Rule Changes are allowed by default. Meaning changes are reverted only when explicitly denied by user interaction through slack channel interactive Deny button. Prerequisites Appropriate IAM Roles and trust relationships within the AWS Organization and member accounts, FleetAccess . CloudTrail Lake setup - Tutorial to enable cloudtrail lake A role in the Organization account with the ability to invoke start_query and get_query_results and trusted by the hub-001 role in the Security account , where the lambda functions run from. Slack App setup with the API Gateway endpoint updated. The Signing secret from the app should be updated in the config file to be used by the lambda function to verify requests from slack. The app enabled with features - Incoming Webhooks, Interactivity. Usage of the latest available boto3 library as API calls related to cloudtrail lake is new and only available on very recent versions, thus boto3 is packaged along with lambda functions. Have Cloud Development Kit installed with npm install cdk . https://github.com/aws/aws-cdk Components Lambda Functions revertsg-1 - Triggered every 10 mins by cloudwatch event rule. revertsg-2 - Invoked by API Gateway. API Gateway to receive requests from Slack and proxy to revertsg-2 Dynamodb to hold security group rule change details. CloudWatch Event Rule time based to trigger revertsg-1 every 10 mins. Workflow CloudWatch Event Time based rule will trigger lambda revertsg-1 , every 10 mins. Lambda function revertsg-1 will assume role cloudtrail-lake-read-role in the organization account and run query to fetch events with event name AuthorizeSecurityGroupIngress in the last 20 mins. There is an over lap so that events which were Query results are gathered and new security group rule changes are added to a dynamodb table secgrouprequests and also details are sent to a slack channel in an interactive message with the ability to either ignore or deny this change. Slack interaction invokes API Gateway which in turn invokes revertsg-2 with all the headers and body proxied. Security group rule changes are allowed by default, so, If the user clicks on Approve (well technically it's already approved :P) , revertsg-2 does the same and responds back with the user name who ignored this change event. If the user clicks Deny , meaning to revert the change, revertsg-2 will, Read dynamodb table with the cloudtrail requestid , get that specific event details, assume spoke-001 role on that account from the security account as hub-001 , invokes the revoke_security_group_ingress API call, responds with the messaged as denied with the user name. Usage Git clone https://github.com/raajheshkannaa/green-stone Update config.py in these 4 places, because both the CDK App and the source code depends on various constants from this configuration file. project-root-folder/config.py project-root-folder/src/revertsg-1/config.py project-root-folder/src/revertsg-2/config.py project-root-folder/stacks/config.py AUTOMATION_ACCOUNT = '<SECURITY ACCOUNT ID OR AUTOMATION ACCOUNT ID>' # Where our automation is run ORG_ACCOUNT = '<ORGANIZATION OR BILLING ACCOUNT ID>' # This is where CloudTrail Lake is setup. #CLOUDTRAIL_LAKE_READ_ROLE = '<ROLE USED FOR RUNNING AND GATHERING CLOUDTRAIL LAKE QUERY RESULTS>' CLOUDTRAIL_LAKE_READ_ROLE = 'cloudtrail-lake-read-role' # This is the role name used if deployed using FleetAccess - https://github.com/raajheshkannaa/fleet-access HOOK_URL = '<SLACK HOOK URL>' # https://api.slack.com/messaging/webhooks SIGNINGSECRET = <THE SECRET FROM SLACK APP USED FOR VERIFICATION OF REQUESTS COMING FROM SLACK> Once the above details are updated, run cdk ls to confirm the stacks are good. Run cdk synth to make sure templates synthesize without errors. With the Security Account credentials in the local terminal using export AWS_PROFILE=<security account creds> , run cdk deploy or use cdk deploy --profile security-account . CDK will deploy the CSGDRRPipelineStack pipeline stack, which in turn creates the CodeCommit repository, CodePipeline with necessary IAM permissions, Stages of deployment. At end of deployment, in the terminal cdk would print out the API Gateway url which needs to be updated in the Slack App in the interactivity section. The first time you deploy, the pipeline runs and fails, because we haven't pushed our code yet. So, now gather the codecommit repository details which was created and git push the cdk app to the repository which has the cloudformation stacks to be deployed by the pipeline. Once code is pushed, pipeline picks it up automatically and begins deployment, its an amazing feeling to look at this in action, with so minimal effort. The automation is triggered every 10 mins and if there are any Security Group changes, that would show up in the Slack Channel for which the webhook was configured earlier. Considerations CloudTrail events are delayed by up to 2-3 mins sometimes before it gets delivered to the cloudtrail lake. Timings are adjusted accordingly for this project, with the CloudWatch Rule and also the event times` for the cloudtrail lake query. Security Group Rule Changes are allowed by default, to make sure Security doesn't add friction to the operations with rest of the organization. Meaning changes are reverted only when explicitly denied by user interaction through slack channel interactive Deny button. Because this is a new service/feature from AWS, Lambda's boto3 library is not updated with the ability to invoke these API calls, so I've packaged a recent version of boto3 part of the lambda function. This could have been a Lambda layer, but hey, doesn't matter after couple weeks or a month. If you liked this, then you can buy me my first coffee ever, THANKS IN ADVANCE","title":"Security Group Change & Response"},{"location":"aws/incident-response/green-stone/#introduction","text":"Get notified of Security Group Changes across all AWS Accounts & Regions in an AWS Organization, with the ability to respond/revert those changes with a single button click from a Slack Channel. This is made easy and possible with the recent announcement of CloudTrail Lake, which helps aggregate CloudTrail logs from all accounts/regions in a queryable(if that's a word :P) format. The infrastructure needed for this project is deployed as a CDK Application, which deploys a CodeCommit repository and a CodeBuild stage which synthesizes the cdk app to cloudformation template and deploys in the target environment. NOTE Security Group Rule Changes are allowed by default. Meaning changes are reverted only when explicitly denied by user interaction through slack channel interactive Deny button.","title":"Introduction"},{"location":"aws/incident-response/green-stone/#prerequisites","text":"Appropriate IAM Roles and trust relationships within the AWS Organization and member accounts, FleetAccess . CloudTrail Lake setup - Tutorial to enable cloudtrail lake A role in the Organization account with the ability to invoke start_query and get_query_results and trusted by the hub-001 role in the Security account , where the lambda functions run from. Slack App setup with the API Gateway endpoint updated. The Signing secret from the app should be updated in the config file to be used by the lambda function to verify requests from slack. The app enabled with features - Incoming Webhooks, Interactivity. Usage of the latest available boto3 library as API calls related to cloudtrail lake is new and only available on very recent versions, thus boto3 is packaged along with lambda functions. Have Cloud Development Kit installed with npm install cdk . https://github.com/aws/aws-cdk","title":"Prerequisites"},{"location":"aws/incident-response/green-stone/#components","text":"Lambda Functions revertsg-1 - Triggered every 10 mins by cloudwatch event rule. revertsg-2 - Invoked by API Gateway. API Gateway to receive requests from Slack and proxy to revertsg-2 Dynamodb to hold security group rule change details. CloudWatch Event Rule time based to trigger revertsg-1 every 10 mins.","title":"Components"},{"location":"aws/incident-response/green-stone/#workflow","text":"CloudWatch Event Time based rule will trigger lambda revertsg-1 , every 10 mins. Lambda function revertsg-1 will assume role cloudtrail-lake-read-role in the organization account and run query to fetch events with event name AuthorizeSecurityGroupIngress in the last 20 mins. There is an over lap so that events which were Query results are gathered and new security group rule changes are added to a dynamodb table secgrouprequests and also details are sent to a slack channel in an interactive message with the ability to either ignore or deny this change. Slack interaction invokes API Gateway which in turn invokes revertsg-2 with all the headers and body proxied. Security group rule changes are allowed by default, so, If the user clicks on Approve (well technically it's already approved :P) , revertsg-2 does the same and responds back with the user name who ignored this change event. If the user clicks Deny , meaning to revert the change, revertsg-2 will, Read dynamodb table with the cloudtrail requestid , get that specific event details, assume spoke-001 role on that account from the security account as hub-001 , invokes the revoke_security_group_ingress API call, responds with the messaged as denied with the user name.","title":"Workflow"},{"location":"aws/incident-response/green-stone/#usage","text":"Git clone https://github.com/raajheshkannaa/green-stone Update config.py in these 4 places, because both the CDK App and the source code depends on various constants from this configuration file. project-root-folder/config.py project-root-folder/src/revertsg-1/config.py project-root-folder/src/revertsg-2/config.py project-root-folder/stacks/config.py AUTOMATION_ACCOUNT = '<SECURITY ACCOUNT ID OR AUTOMATION ACCOUNT ID>' # Where our automation is run ORG_ACCOUNT = '<ORGANIZATION OR BILLING ACCOUNT ID>' # This is where CloudTrail Lake is setup. #CLOUDTRAIL_LAKE_READ_ROLE = '<ROLE USED FOR RUNNING AND GATHERING CLOUDTRAIL LAKE QUERY RESULTS>' CLOUDTRAIL_LAKE_READ_ROLE = 'cloudtrail-lake-read-role' # This is the role name used if deployed using FleetAccess - https://github.com/raajheshkannaa/fleet-access HOOK_URL = '<SLACK HOOK URL>' # https://api.slack.com/messaging/webhooks SIGNINGSECRET = <THE SECRET FROM SLACK APP USED FOR VERIFICATION OF REQUESTS COMING FROM SLACK> Once the above details are updated, run cdk ls to confirm the stacks are good. Run cdk synth to make sure templates synthesize without errors. With the Security Account credentials in the local terminal using export AWS_PROFILE=<security account creds> , run cdk deploy or use cdk deploy --profile security-account . CDK will deploy the CSGDRRPipelineStack pipeline stack, which in turn creates the CodeCommit repository, CodePipeline with necessary IAM permissions, Stages of deployment. At end of deployment, in the terminal cdk would print out the API Gateway url which needs to be updated in the Slack App in the interactivity section. The first time you deploy, the pipeline runs and fails, because we haven't pushed our code yet. So, now gather the codecommit repository details which was created and git push the cdk app to the repository which has the cloudformation stacks to be deployed by the pipeline. Once code is pushed, pipeline picks it up automatically and begins deployment, its an amazing feeling to look at this in action, with so minimal effort. The automation is triggered every 10 mins and if there are any Security Group changes, that would show up in the Slack Channel for which the webhook was configured earlier.","title":"Usage"},{"location":"aws/incident-response/green-stone/#considerations","text":"CloudTrail events are delayed by up to 2-3 mins sometimes before it gets delivered to the cloudtrail lake. Timings are adjusted accordingly for this project, with the CloudWatch Rule and also the event times` for the cloudtrail lake query. Security Group Rule Changes are allowed by default, to make sure Security doesn't add friction to the operations with rest of the organization. Meaning changes are reverted only when explicitly denied by user interaction through slack channel interactive Deny button. Because this is a new service/feature from AWS, Lambda's boto3 library is not updated with the ability to invoke these API calls, so I've packaged a recent version of boto3 part of the lambda function. This could have been a Lambda layer, but hey, doesn't matter after couple weeks or a month.","title":"Considerations"},{"location":"aws/incident-response/green-stone/#if-you-liked-this-then-you-can-buy-me-my-first-coffee-ever-thanks-in-advance","text":"","title":"If you liked this, then you can buy me my first coffee ever, THANKS IN ADVANCE"},{"location":"aws/logging-and-monitoring/asm/","text":"AWS Attack Surface Management Continuous AWS Attack Surface Discovery of external facing services and Scanning using Nmap & Vulners' scripts. If you look at this in the right angle , technically this is an External Vulnerability Analysis at scale in a tight budget. Pre-requsites Depends on the presense of an IAM Role structure such as defined by Fleet Access , where in we have a central automation account with a role with trust relationship with a role in every AWS Account. If you use your own IAM Role structure in a multi-account strategy, please ensure to update the necessary code where src role is used for the Lambda functions. Infrastructure The infrastructure for this project is built with Cloud Development Kit or CDK. The primary reason is the self mutating capability of CDK Pipelines, secondary because the source for AAS Management is built with Python, building Infrastructure in the same language is a huge plus. If you don't have CDK setup locally already before deployment, please refer to the Official CDK page or install by running below command in a terminal. npm i -g aws-cdk Components CodeCommit repository AttackSurfaceManagement is created part of the pipeline. CloudWatch Event - Time Based Cron is scheduled to be triggered midnight UTC Lambda Functions - Deployed in a single stage ASM-1 - Gathers AWS Accounts dynamically from AWS Organizations ASM-2 - Uses boto3 to gather external IP address from Lightsail, RDS, ELBv1/2, Elasticsearch, Redshift, CloudFront, EC2, Beanstalk, API Gateway ASM-3(Docker Container) - Image built with nmap to scan the external IP Address from those services. Roles The entire project is built over existing Trust Relationship of roles in the respective AWS Accounts. Refer Project FleetAccess for more information. Hub Role - present in the Security or Automation Account Org Read Only Role - present in the Billing/AWS Organizations Account with a trust relationship with the Hub Role Spoke Role - present in all AWS Accounts or commonly referred as the fleet, with trust relationthip with the Hub Role. VPC Lambda functions are built in a VPC with a private and public subnet. NAT Gateway in the public subnet. Lambda functions in the private subnet routing egress traffic through the NAT Gateway. There are no security groups for ingress, as traffic is not expected. - AWS Cross Account Access at Scale: fleetaccess.md Usage git clone https://github.com/raajheshkannaa/attack-surface-management Open config.py and update AUTOMATION_ACCOUNT , ORG_ACCOUNT and HOOK_URL . Automation account is the central account where the Hub IAM role with trust relationships' with rest of the fleet accounts using Spoke IAM role is already established. Note : if you are using your own IAM role setup, please update the pipeline stack accordingly. Organiztion account is the billing account id. HOOK_URL is the slack hook url of the channel to which messages will be post. Once updated, run cdk ls , this should list the stacks which will deployed. In our case it's the pipeline stack. Assuming, automation account credentials are setup in a terminal using export AWS_PROFILE=aws-automation-creds , deploy the stack using cdk deploy . This will deploy the pipeline stack and run for the first time, which will fail since we haven't pushed the code yet. The pipeline would have created the CodeCommit repository. After acquiring the repo details, git push cdk app to the repository. This will trigger the pipeline and deploy the LambdaDeploymentStack which consits for the Event trigger, VPC, Subnets, Lambda functions. Once the infrastructure is setup, Attack Surface Discovery is inititated at midnight UTC, which will gather information from all accounts' which has the Spoke role deployed and notify of open ports other than port {80,443} in the slack channel mentioned. Workflow A time based CloudWatch Event triggers ASM-1 lambda function which assumes the org_read_only_role in the Billing Account to gather list of AWS Accounts and invokes the ASM-2 . ASM-1 invokes ASM-2 in a for loop passing the AWS Account IDs as the payload, essentially making it a asynchronuous invocation of the ASM-2, the n number of times the number of AWS Accounts. For instance if there are 300 AWS Accounts, ASM-1 invokes ASM-2, 300 times with respective AWS Accounts IDs. ASM-2 does the heavy lifting of enumerating 10 different AWS Services which could have an external presense. ASM-2 invokes ASM-3 for every external IP Address in every service as payload. ASM-3 is a docker container run in Lambda which has nmap packaged. This is again is asynchronously invoked, meaning ASM-3 is invoked as many times as the number of external IP Addresses for each service. This is extremely scalable with any number of AWS Accounts and external facing services. After the IP address is scanned with the Vulners scripts`, notifications on information about open ports are sent to a slack channel. To be Added (in the next version) Add capability to store nmap scan results to a S3 bucket, for nmap diffs, so that only changes are reported or notified. This could then be imported into DefectDojo for a complete Vulnerability Management solution. For more ideas see Advanced section. Add capability to store IP Addresses and open ports information in a DynamoDB Table, so that this informaiton is looked up before notifications, this way same alerts are not sent again. -- Done Why not Some of the questions I think of, why not just use AWS Inspector's Network Reachability module to help in identifying the open ports. The answer is that, it only helps in the case of EC2 instances, the same couldn't be used for public RDS instances or Elasticsearch instances. Follow up question to that answer is, why not just alert on those specific API calls when, say a RDS or Elasticsearc instance is made public. The answer is not simple, because for one reason, it's cumbersome to find existing public resources, while also some of the API calls are a bit complicated when it comes to identifying if a resource is being made public. If you liked this, then you can buy me my first coffee ever, THANKS IN ADVANCE","title":"Mapping the AWS Attack Surface"},{"location":"aws/logging-and-monitoring/asm/#aws-attack-surface-management","text":"Continuous AWS Attack Surface Discovery of external facing services and Scanning using Nmap & Vulners' scripts. If you look at this in the right angle , technically this is an External Vulnerability Analysis at scale in a tight budget.","title":"AWS Attack Surface Management"},{"location":"aws/logging-and-monitoring/asm/#pre-requsites","text":"Depends on the presense of an IAM Role structure such as defined by Fleet Access , where in we have a central automation account with a role with trust relationship with a role in every AWS Account. If you use your own IAM Role structure in a multi-account strategy, please ensure to update the necessary code where src role is used for the Lambda functions.","title":"Pre-requsites"},{"location":"aws/logging-and-monitoring/asm/#infrastructure","text":"The infrastructure for this project is built with Cloud Development Kit or CDK. The primary reason is the self mutating capability of CDK Pipelines, secondary because the source for AAS Management is built with Python, building Infrastructure in the same language is a huge plus. If you don't have CDK setup locally already before deployment, please refer to the Official CDK page or install by running below command in a terminal. npm i -g aws-cdk","title":"Infrastructure"},{"location":"aws/logging-and-monitoring/asm/#components","text":"CodeCommit repository AttackSurfaceManagement is created part of the pipeline. CloudWatch Event - Time Based Cron is scheduled to be triggered midnight UTC Lambda Functions - Deployed in a single stage ASM-1 - Gathers AWS Accounts dynamically from AWS Organizations ASM-2 - Uses boto3 to gather external IP address from Lightsail, RDS, ELBv1/2, Elasticsearch, Redshift, CloudFront, EC2, Beanstalk, API Gateway ASM-3(Docker Container) - Image built with nmap to scan the external IP Address from those services. Roles The entire project is built over existing Trust Relationship of roles in the respective AWS Accounts. Refer Project FleetAccess for more information. Hub Role - present in the Security or Automation Account Org Read Only Role - present in the Billing/AWS Organizations Account with a trust relationship with the Hub Role Spoke Role - present in all AWS Accounts or commonly referred as the fleet, with trust relationthip with the Hub Role. VPC Lambda functions are built in a VPC with a private and public subnet. NAT Gateway in the public subnet. Lambda functions in the private subnet routing egress traffic through the NAT Gateway. There are no security groups for ingress, as traffic is not expected. - AWS Cross Account Access at Scale: fleetaccess.md","title":"Components"},{"location":"aws/logging-and-monitoring/asm/#usage","text":"git clone https://github.com/raajheshkannaa/attack-surface-management Open config.py and update AUTOMATION_ACCOUNT , ORG_ACCOUNT and HOOK_URL . Automation account is the central account where the Hub IAM role with trust relationships' with rest of the fleet accounts using Spoke IAM role is already established. Note : if you are using your own IAM role setup, please update the pipeline stack accordingly. Organiztion account is the billing account id. HOOK_URL is the slack hook url of the channel to which messages will be post. Once updated, run cdk ls , this should list the stacks which will deployed. In our case it's the pipeline stack. Assuming, automation account credentials are setup in a terminal using export AWS_PROFILE=aws-automation-creds , deploy the stack using cdk deploy . This will deploy the pipeline stack and run for the first time, which will fail since we haven't pushed the code yet. The pipeline would have created the CodeCommit repository. After acquiring the repo details, git push cdk app to the repository. This will trigger the pipeline and deploy the LambdaDeploymentStack which consits for the Event trigger, VPC, Subnets, Lambda functions. Once the infrastructure is setup, Attack Surface Discovery is inititated at midnight UTC, which will gather information from all accounts' which has the Spoke role deployed and notify of open ports other than port {80,443} in the slack channel mentioned.","title":"Usage"},{"location":"aws/logging-and-monitoring/asm/#workflow","text":"A time based CloudWatch Event triggers ASM-1 lambda function which assumes the org_read_only_role in the Billing Account to gather list of AWS Accounts and invokes the ASM-2 . ASM-1 invokes ASM-2 in a for loop passing the AWS Account IDs as the payload, essentially making it a asynchronuous invocation of the ASM-2, the n number of times the number of AWS Accounts. For instance if there are 300 AWS Accounts, ASM-1 invokes ASM-2, 300 times with respective AWS Accounts IDs. ASM-2 does the heavy lifting of enumerating 10 different AWS Services which could have an external presense. ASM-2 invokes ASM-3 for every external IP Address in every service as payload. ASM-3 is a docker container run in Lambda which has nmap packaged. This is again is asynchronously invoked, meaning ASM-3 is invoked as many times as the number of external IP Addresses for each service. This is extremely scalable with any number of AWS Accounts and external facing services. After the IP address is scanned with the Vulners scripts`, notifications on information about open ports are sent to a slack channel.","title":"Workflow"},{"location":"aws/logging-and-monitoring/asm/#to-be-added-in-the-next-version","text":"Add capability to store nmap scan results to a S3 bucket, for nmap diffs, so that only changes are reported or notified. This could then be imported into DefectDojo for a complete Vulnerability Management solution. For more ideas see Advanced section. Add capability to store IP Addresses and open ports information in a DynamoDB Table, so that this informaiton is looked up before notifications, this way same alerts are not sent again. -- Done","title":"To be Added (in the next version)"},{"location":"aws/logging-and-monitoring/asm/#why-not","text":"Some of the questions I think of, why not just use AWS Inspector's Network Reachability module to help in identifying the open ports. The answer is that, it only helps in the case of EC2 instances, the same couldn't be used for public RDS instances or Elasticsearch instances. Follow up question to that answer is, why not just alert on those specific API calls when, say a RDS or Elasticsearc instance is made public. The answer is not simple, because for one reason, it's cumbersome to find existing public resources, while also some of the API calls are a bit complicated when it comes to identifying if a resource is being made public.","title":"Why not"},{"location":"aws/logging-and-monitoring/asm/#if-you-liked-this-then-you-can-buy-me-my-first-coffee-ever-thanks-in-advance","text":"","title":"If you liked this, then you can buy me my first coffee ever, THANKS IN ADVANCE"},{"location":"aws/logging-and-monitoring/cloudtrail-lake-detections/","text":"CloudTrail Lake Threat Detections Threat detections you can enable RIGHT NOW with the capability of AWS CloudTrail Lake. All you need is to toggle couple switches in the Organization Account to enable CloudTrail Lake, which will collect cloudtrail logs from all accounts & regions and normalize those logs for us to query using sql. We will use lambda functions` to query the lake for threats and alert slack. Previously to do this at scale meant, enabling CloudTrail in all accounts and regions, sending those logs to S3 or a data lake has to be configured with partitions setup accordingly and athena queries to be scheduled. There are multiple moving parts to this equation, however with the announcement of CloudTrail Lake, all of this is extremely straight forward. Note : Detections forked from Panther Labs CloudTrail Rules - https://github.com/panther-labs/panther-analysis/tree/master/aws_cloudtrail_rules Detections - ami_modified_for_public_image - resource_made_public - snapshot_made_public - key_compromised - security_configuration_change - codebuild_made_public If you liked this, then you can buy me my first coffee ever, THANKS IN ADVANCE","title":"CloudTrail Lake Detections"},{"location":"aws/logging-and-monitoring/cloudtrail-lake-detections/#cloudtrail-lake-threat-detections","text":"Threat detections you can enable RIGHT NOW with the capability of AWS CloudTrail Lake. All you need is to toggle couple switches in the Organization Account to enable CloudTrail Lake, which will collect cloudtrail logs from all accounts & regions and normalize those logs for us to query using sql. We will use lambda functions` to query the lake for threats and alert slack. Previously to do this at scale meant, enabling CloudTrail in all accounts and regions, sending those logs to S3 or a data lake has to be configured with partitions setup accordingly and athena queries to be scheduled. There are multiple moving parts to this equation, however with the announcement of CloudTrail Lake, all of this is extremely straight forward. Note : Detections forked from Panther Labs CloudTrail Rules - https://github.com/panther-labs/panther-analysis/tree/master/aws_cloudtrail_rules","title":"CloudTrail Lake Threat Detections"},{"location":"aws/logging-and-monitoring/cloudtrail-lake-detections/#detections","text":"- ami_modified_for_public_image - resource_made_public - snapshot_made_public - key_compromised - security_configuration_change - codebuild_made_public","title":"Detections"},{"location":"aws/logging-and-monitoring/cloudtrail-lake-detections/#if-you-liked-this-then-you-can-buy-me-my-first-coffee-ever-thanks-in-advance","text":"","title":"If you liked this, then you can buy me my first coffee ever, THANKS IN ADVANCE"},{"location":"blog/","text":"Space where I rant about security in the cloud","title":"Blog"},{"location":"blog/i-know-terraform-but-i-build-with-cdk/","text":"","title":"I know terraform but i build with cdk"},{"location":"blog/new_blog_post/","text":"New Blog","title":"New Post"},{"location":"blog/new_blog_post/#new-blog","text":"","title":"New Blog"},{"location":"misc/","text":"Collections of random scripts and automations used for various purposes","title":"Misc"}]}